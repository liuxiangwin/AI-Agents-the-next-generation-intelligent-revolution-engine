{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766aaa81-96e6-42dc-b29d-8216d2a7feec",
   "metadata": {},
   "source": [
    "## Agentic AI (ReAct) with LangGraph\n",
    "\n",
    "### LLM Used - Granite3.1-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544e6b6",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to create and deploy a ReAct (Reasoning + Acting) agent capable of solving complex tasks by combining logical reasoning with external tool usage. The agent will use a structured loop of Thought → Action → Observation to iteratively reason through problems, gather data, and provide solutions.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "* Implement a ReAct agent that breaks down complex user queries into smaller, manageable steps.\n",
    "* Integrate external tools (e.g., search engines, calculators, etc) that the agent can use to enhance its problem-solving abilities.\n",
    "* Efficiently handle tool outputs and errors, allowing the agent to adapt its actions based on real-time feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609e856",
   "metadata": {},
   "source": [
    "> NOTE: This notebook requires vLLM 0.6.4 that includes [Tool Calling / Function Calling](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#ibm-granite).\n",
    "    A custom Service Runtime is used with the following parameters:\n",
    "```\n",
    "    - --enable-auto-tool-choice\n",
    "    - --tool-call-parser\n",
    "    - \"granite\"\n",
    "    - --chat-template\n",
    "    - \"/app/data/template/tool_chat_template_granite.jinja\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8866d",
   "metadata": {},
   "source": [
    "### 1. Agentic AI Overview \n",
    "\n",
    "An Agent is an autonomous system that leverages Large Language Models (LLMs) to perform tasks by understanding, reasoning, planning, and executing actions with minimal human intervention. AI agents are designed to break down complex problems into manageable steps, utilizing tools, accessing memory, and adapting their behavior based on the provided context.\n",
    "\n",
    "At its core, an agent is structured to:\n",
    "\n",
    "* Receive a Task: The agent takes input from the user, such as a question or command.\n",
    "* Plan a Solution: The agent decomposes the problem, chooses appropriate tools, and reasons through possible solutions.\n",
    "* Execute the Plan: It performs actions, such as retrieving information, using tools, or generating responses based on the devised plan.\n",
    "* Deliver Results: Finally, it presents the solution or output in a structured, actionable format.\n",
    "\n",
    "![LLM Tools](../../docs/images/04-02-react-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c92f42",
   "metadata": {},
   "source": [
    "### 2. Agentic AI Framework Used - LangGraph\n",
    "\n",
    "**LangGraph** is a library designed for creating complex, stateful applications that use large language models (LLMs) to manage agent and multi-agent workflows. Unlike traditional frameworks, LangGraph uses graph-based cycles instead of directed acyclic graphs (DAGs), enabling more intricate and adaptable workflows. This approach supports advanced use cases where agents may revisit tasks or adjust based on dynamic changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f3fb-ee50-40f2-b276-b8194668e49e",
   "metadata": {},
   "source": [
    "### 3. Setup and Import Libraries\n",
    "\n",
    "To get started, you'll need to install and import a few Python libraries. Run the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install -q langgraph==0.2.35 langchain_experimental==0.0.65 langchain-openai==0.1.25 termcolor==2.3.0 duckduckgo_search==7.1.0 openapi-python-client==0.12.3 langchain_community==0.2.19 wikipedia==1.4.0 termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebef994b-baf1-4afd-90a9-9a8a62946717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install duckduckgo_search==7.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bb3f0f-40b5-49a6-b493-5e361db0113e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7c62-ea7d-4fd3-adcf-847beee5c0fb",
   "metadata": {},
   "source": [
    "### 4. Model Configuration\n",
    "\n",
    "We will start by creating an llm instance, defined by the location where the LLM API can be queried and some parameters that will be applied to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bf848-656e-49ee-bc1e-7c4d2474678d",
   "metadata": {},
   "source": [
    "#### 4.1 Define the Inference Model Server specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b908fd0-01dd-4ad2-b745-b3a4c56a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_URL = os.getenv('API_URL_GRANITE')\n",
    "MODEL_NAME = \"granite-3-8b-instruct\"\n",
    "API_KEY= os.getenv('API_KEY_GRANITE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b2f3f-ac23-4531-984b-6e8357233992",
   "metadata": {},
   "source": [
    "#### 4.2 Create the LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01baa2b8-529d-455d-ad39-ef4a96dbaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70132439-184c-46bc-b3aa-098bd5310c1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Define tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffca944-5ef0-430a-9abf-6fe93007e091",
   "metadata": {},
   "source": [
    "First, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example.\n",
    "\n",
    "On the other hand sometimes, for complex calculations, rather than have an LLM generate the answer directly, it can be better to have the LLM generate code to calculate the answer, and then run that code to get the answer. In order to easily do that, we provide a simple Python REPL to execute commands in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192a9964-44cb-4095-8c4b-bb7e36753b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your calculations.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb956c",
   "metadata": {},
   "source": [
    "We will define the list of Tools that will be used by our AI Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81154c2a-abe2-42cd-88f7-9777007895ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [duckduckgo_search, python_repl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5ee2f",
   "metadata": {},
   "source": [
    "We need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the .bind_tools() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f30dac-6b3f-4731-8640-47dca14aeb11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b4553-ec99-4e9b-ab49-0836bc7b186a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Creating a ReAct Agent\n",
    "\n",
    "In this section, we will build a ReAct agent that autonomously reasons through tasks and selects the appropriate tools to solve problems. The agent will follow a structured process to deliver actionable results.\n",
    "\n",
    "#### 6.1 Architecture Explanation\n",
    "\n",
    "- **Agent Components**:\n",
    "  - **Memory**: The agent's memory module stores past interactions and relevant data to inform future decisions.\n",
    "  - **Planning**: This module breaks down complex tasks into manageable steps, helping the agent decide the sequence of actions.\n",
    "  - **Tools**: The tools module enables the agent to interact with external systems, such as performing calculations, retrieving data, or running APIs.\n",
    "\n",
    "- **System Prompt & User Prompt**: \n",
    "  - The **System Prompt** provides context and instructions for how the agent should operate within its environment, including the tools available.\n",
    "  - The **User Prompt** is the input provided by the user, specifying the task or query the agent needs to handle.\n",
    "\n",
    "- **LLM Interaction**: \n",
    "  - The Large Language Model (LLM) is at the core of the agent’s decision-making. It processes both the system and user prompts, and its outputs guide the agent’s actions. The LLM can reason about tasks, plan, and adapt based on external tool outputs.\n",
    "  \n",
    "- **External Environments**:\n",
    "  - The **Local Environment** consists of tools and systems the agent interacts with locally on its workstation.\n",
    "  - The **External Environment** is any external system or API that the agent can call for additional information or actions.\n",
    "\n",
    "This architecture allows the ReAct agent to efficiently break down complex tasks, take action, and observe the results in a structured, repeatable process.\n",
    "\n",
    "![Agent Architecture](../../docs/images/05-03-react-agent-architecture-modules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89602f",
   "metadata": {},
   "source": [
    "#### 6.2 Create Agent Graphs\n",
    "\n",
    "Create the individual agents below and tell them how to talk to each other using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d96fe7-b74d-4e93-af91-5ba2c5242fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    AIMessage,  # Make sure to import AIMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0939f8-7149-492b-8a36-fc09b838c54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6.3 Define Edge Logic\n",
    "\n",
    "We can define some of the edge logic that is needed to decide what to do based on results of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08b4c68-11b2-4407-8732-3093fbe4cf32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128d826-bb1c-474c-b803-4f8ee93bf5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cc159-a794-4304-922d-b46ac393266d",
   "metadata": {},
   "source": [
    "#### 6.4 Define Tool Node\n",
    "\n",
    "Defines the logic for the graph. Adds conditional edges for the chatbot to use tools when needed, and ensures the flow returns to the chatbot after tool execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d03d3-cbe3-42ac-8a90-3ef612d57c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97913aee",
   "metadata": {},
   "source": [
    "Sets up a memory-saving mechanism to persist agent interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ae4e2f-789e-43c0-93a7-30e4e74d4fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff378",
   "metadata": {},
   "source": [
    "#### 6.5 System Prompt\n",
    "\n",
    "The system prompt provides the instructions that guide the agent in reasoning through tasks, using tools, and generating structured responses. It establishes the context, including the environment (e.g., ipython) and knowledge cut-off date (December 2023), ensuring the agent understands the limits of its information.\n",
    "\n",
    "The agent is tasked with using tools to solve problems and must decide which tool to use and in what sequence. Each interaction follows a structured JSON format, ensuring clarity in both tool inputs and outputs.\n",
    "\n",
    "The agent operates in a cycle of **thought → action → observation**:\n",
    "- **Thought**: The agent thinks about the task and determines the next action.\n",
    "- **Action**: The agent selects and uses the appropriate tool.\n",
    "- **Observation**: The agent analyzes the tool's result and decides the next step.\n",
    "\n",
    "This process repeats until the agent reaches a sufficient conclusion to answer the user’s query. If the tool provides a clear result, the agent will stop further actions and present the final answer. If the task cannot be completed, the agent will explain the limitation and provide suggestions.\n",
    "\n",
    "The system prompt ensures that the agent behaves logically, utilizes tools efficiently, and delivers structured and coherent responses.\n",
    "\n",
    "![Agent Architecture](../../docs/images/05-04-system-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f63d4",
   "metadata": {},
   "source": [
    "Compiles the state graph with the memory saver for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f4155-3efb-45e7-9bdc-9adfb7726ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfbd1dc6-8d9e-4b89-bcfe-f2e465af3f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sets up a basic configuration for the graph execution.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727122",
   "metadata": {},
   "source": [
    "### 7. Using React Agents with Complex Queries\n",
    "\n",
    "In this section, we will execute the `ReActAgent` to demonstrate how it processes user input, interacts with tools, and follows the **Thought → Action → Observation** loop to produce a final answer.\n",
    "\n",
    "### Steps to Run the Agent:\n",
    "\n",
    "1. **Initialize the Agent**: \n",
    "   - We start by setting up the initial state, defining the agent's role, specifying the available tools, and connecting the agent to a language model service.\n",
    "   \n",
    "2. **User Request**: \n",
    "   - A user query is provided as input, such as \"What is the sum of 15 and 27?\" The agent receives this request and starts reasoning through it.\n",
    "\n",
    "3. **Processing the Request**:\n",
    "   - The agent constructs a system prompt with the user’s input and interacts with the language model in a continuous loop, checking for actions or final answers.\n",
    "   - If a tool is needed (e.g., a calculator), the agent will invoke the appropriate tool, observe the result, and feed the output back into its reasoning process.\n",
    "\n",
    "4. **Final Output**:\n",
    "   - The agent continues looping through the **Thought → Action → Observation** cycle until a final answer is generated or the task cannot be completed. Once complete, the result is stored in the agent’s state, and the final answer is returned.\n",
    "\n",
    "By the end of this process, the agent will have used reasoning and external tools to autonomously solve the problem presented by the user.\n",
    "\n",
    "![Agent Architecture](../../docs/images/05-05-react-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "321f2258-0334-4bb4-b3c0-781311c143c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize last_message_index outside the function to persist across interactions\n",
    "last_message_index = 0\n",
    "\n",
    "def react_agent(user_input):\n",
    "    global last_message_index\n",
    "    # Start streaming the conversation\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "    # Process each event and print only new messages\n",
    "    for event in events:\n",
    "        messages = event[\"messages\"]\n",
    "\n",
    "        # Get new messages since last processed index\n",
    "        new_messages = messages[last_message_index:]\n",
    "        for message in new_messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                print(\"==================================\\u001b[1m Agent Message \\u001b[0m==================================\\n\")\n",
    "                print(message.content)\n",
    "        # Update the last_message_index\n",
    "        last_message_index += len(new_messages)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a7075",
   "metadata": {},
   "source": [
    "### 7.1 ReAct in Action!\n",
    "\n",
    "Start a complex user query to the agent. \n",
    "\n",
    "We want to **calculate the distance from the nearest city of the biggest lake in the world**. Challenging, right?\n",
    "For a regular LLM it is, but we're using ReAct AI Agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfca23-56a3-43f5-b0a0-f932f7d7e328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First Question\n",
    "react_agent(\"What is the biggest mountain in the world?\")\n",
    "\n",
    "# Second Question\n",
    "react_agent(\"Obtain the area of the mountain\")\n",
    "\n",
    "# Third Question\n",
    "react_agent(\"Now calculate the distance between the nearest city of the mountain. Select the closest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92689550-c1d6-4e4c-91ff-744b58c20bf6",
   "metadata": {},
   "source": [
    "1. **The agent searches for the biggest mountain using DuckDuckGo**  \n",
    "   - The agent identifies Mount Everest as the result and notes its height as 8,848.86 meters (29,031.7 feet) above sea level.\n",
    "\n",
    "2. **The agent calculates the area of Mount Everest**  \n",
    "   - Assuming it is a perfect cone with a base radius of 6,052.8 meters, the agent uses a Python calculator tool to determine the area as approximately **1.08 million square kilometers**.\n",
    "\n",
    "3. **The agent identifies the closest city to Mount Everest**  \n",
    "   - Using DuckDuckGo, the agent finds that Lhasa, is the closest city to Mount Everest.\n",
    "\n",
    "4. **The agent calculates the distance from Lasha to Mount Everest**  \n",
    "   - By applying the Haversine formula, the agent determines the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d546a2-4eb4-4ccb-8afd-546d20f45f6b",
   "metadata": {},
   "source": [
    "#### 7.2 Chaining the questions for ReAct Agents\n",
    "\n",
    "Now that we asked several complex questions let's see if chaining the questions in just one prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7db0ed-c3ff-495b-a0a3-74e9bf524e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent(\"\"\"\n",
    "What is the biggest mountain in the world?\n",
    "Obtain the area of the mountain\n",
    "Now calculate the distance between the nearest city of the mountain. Select the closest\n",
    "Can I climb it with Flip Flops? Which shoes do recommend me to wear?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de43ea2-f335-4c2b-aab0-c7838061dbce",
   "metadata": {},
   "source": [
    "Chaining multiple complex questions in a single prompt allows the ReAct Agent to provide a concise, accurate summary without detailing much each step of the process. \n",
    "\n",
    "The agent combines its reasoning to directly address the queries, identifying Mount Everest as the biggest mountain, calculating its area as approximately 1.08 million square kilometers, and determining that the closest city, Lasha, is about 190 kilometers away (approx). \n",
    "And obviously thinks that we **should NOT** climb the Everest with Flip Flops, giving us some tips.\n",
    "\n",
    "This approach provides a **streamlined response, focusing on the final answers rather than the intermediate steps**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b8e1c-b797-4a93-a96b-bc31ac62024f",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "* This notebook demonstrates the power of ReAct Agents in solving complex, multi-step queries by integrating reasoning with tool usage. \n",
    "* By chaining prompts or breaking them into steps, the agent efficiently provides accurate, actionable responses. \n",
    "* The use of external tools and dynamic decision-making showcases their adaptability. \n",
    "* This approach streamlines problem-solving, enhances user interactions, and highlights the versatility of LangGraph for building intelligent agents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
