{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46949622-b056-441f-8bc0-29c23f85bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph==0.2.35 langchain_experimental==0.0.65 langchain-openai==0.1.25 termcolor==2.3.0 duckduckgo_search==7.1.0 openapi-python-client==0.12.3 langchain_community==0.2.19 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9b45f1-6737-4e10-83be-91d225fcfd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== \u001b[1mAgent Response\u001b[0m =====\n",
      " <tool_call> \n",
      "\n",
      "\n",
      "===== \u001b[1mAgent Response\u001b[0m =====\n",
      " The Pont des Arts is a bridge in Paris, France. Its length is approximately 330 meters. A leopard can run at a top speed of about 36 miles per hour (58 km/h). \n",
      "\n",
      "To calculate the time it would take for a leopard to run through the bridge, we need to convert the bridge's length into feet and then into miles, as the leopard's speed is given in miles per hour. \n",
      "\n",
      "1 meter = 3.28084 feet\n",
      "330 meters = 1079.46 feet\n",
      "\n",
      "Now, converting feet to miles:\n",
      "1 mile = 5280 feet\n",
      "1079.46 feet = 0.2045 miles\n",
      "\n",
      "Using the formula Time = Distance / Speed, we get:\n",
      "Time = 0.2045 miles / 36 mph = 0.00568 hours\n",
      "\n",
      "To convert this into seconds:\n",
      "0.00568 hours * 3600 seconds/hour = 20.448 seconds\n",
      "\n",
      "So, it would take approximately 20.45 seconds for a leopard running at full speed to cross the Pont des Arts. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Environment Variables\n",
    "INFERENCE_SERVER_URL = os.getenv(\"API_URL_GRANITE\")\n",
    "API_KEY = os.getenv(\"API_KEY_GRANITE\")\n",
    "MODEL_NAME = \"granite-3-8b-instruct\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base=f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Initialize Tools\n",
    "tools = [DuckDuckGoSearchRun()]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define State\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Build Graph\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}  # Append response to message history\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Agent Function\n",
    "def react_agent(user_input):\n",
    "    events = graph.stream({\"messages\": [(HumanMessage(content=user_input))]}, config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        messages = event[\"messages\"]\n",
    "\n",
    "        # Get only the last AI response (avoiding repeated prints)\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, AIMessage):  # Ensure it's the LLM response\n",
    "            print(\"\\n===== \\033[1mAgent Response\\033[0m =====\\n\", last_message.content, \"\\n\")\n",
    "\n",
    "# Example Usage\n",
    "react_agent(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f6b5e-9c48-4d22-898f-c8b799986096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
